{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow has access to the following devices:\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "TensorFlow version: 2.8.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(f\"TensorFlow has access to the following devices:\\n{tf.config.list_physical_devices()}\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import (MultipleLocator, AutoMinorLocator)\n",
    "import seaborn as sns\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20383, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fold</th>\n",
       "      <th>no</th>\n",
       "      <th>clay</th>\n",
       "      <th>silt</th>\n",
       "      <th>sand</th>\n",
       "      <th>BD</th>\n",
       "      <th>omc</th>\n",
       "      <th>pF</th>\n",
       "      <th>VWC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>32.6</td>\n",
       "      <td>43.3</td>\n",
       "      <td>24.1</td>\n",
       "      <td>1.14</td>\n",
       "      <td>4.50</td>\n",
       "      <td>0.841595</td>\n",
       "      <td>0.558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>97</td>\n",
       "      <td>19.8</td>\n",
       "      <td>67.0</td>\n",
       "      <td>13.2</td>\n",
       "      <td>1.14</td>\n",
       "      <td>3.56</td>\n",
       "      <td>1.069393</td>\n",
       "      <td>0.487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>166</td>\n",
       "      <td>27.7</td>\n",
       "      <td>41.6</td>\n",
       "      <td>30.7</td>\n",
       "      <td>1.15</td>\n",
       "      <td>3.00</td>\n",
       "      <td>2.057260</td>\n",
       "      <td>0.394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1015</td>\n",
       "      <td>45.7</td>\n",
       "      <td>30.4</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.49</td>\n",
       "      <td>1.360000</td>\n",
       "      <td>0.620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1066</td>\n",
       "      <td>13.5</td>\n",
       "      <td>25.5</td>\n",
       "      <td>61.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>1.27</td>\n",
       "      <td>2.690000</td>\n",
       "      <td>0.250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fold    no  clay  silt  sand    BD   omc        pF    VWC\n",
       "0     1    55  32.6  43.3  24.1  1.14  4.50  0.841595  0.558\n",
       "1     1    97  19.8  67.0  13.2  1.14  3.56  1.069393  0.487\n",
       "2     1   166  27.7  41.6  30.7  1.15  3.00  2.057260  0.394\n",
       "3     1  1015  45.7  30.4  24.0  0.85  0.49  1.360000  0.620\n",
       "4     1  1066  13.5  25.5  61.0  1.23  1.27  2.690000  0.250"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tureky soil samples are the soils starting with 'no' 1000 or above\n",
    "data = pd.read_csv('data.csv') \\\n",
    "            .drop('hhpa', axis=1)\n",
    "data['VWC'] = data['VWC']/100\n",
    "print(data.shape) \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby('fold').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(18, 10))\n",
    "sns.boxplot(data=data.iloc[:, 2:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.reset_orig()\n",
    "plt.figure(figsize=(18,12))\n",
    "corr = data.iloc[:, 2:].corr()\n",
    "# Getting the Upper Triangle of the co-relation matrix\n",
    "matrix = np.triu(corr)\n",
    "\n",
    "sns.heatmap(corr, annot = True, mask=matrix,cmap=\"YlGnBu\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.pairplot(data.iloc[:, 2:]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_tuner.tuners import RandomSearch \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make directory for storing saved models\n",
    "# os.makedirs('model4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the `bagging` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagging(rawdata):\n",
    "    \"\"\"Take bootstrap samples - sampling with replacement\"\"\"\n",
    "    ###generator random sample id with replacement\n",
    "    idx = pd.DataFrame(rawdata[['no']])\n",
    "    idx_rmdup = idx.drop_duplicates()\n",
    "    length = len(idx_rmdup)\n",
    "    samples = idx_rmdup.sample(length, replace= True)\n",
    "    # changing the sample id into one dimensional list in order to label sampleid in idx \n",
    "    samples_list = samples.transpose().values.tolist()[0] \n",
    "    ###going through the sampleid dataframe and marking the sample as training or validation dataset. \n",
    "    idx['labels'] = ['training' if x in samples_list else 'validation'  for x in idx['no'] ]\n",
    "    rawdata_con = pd.concat([rawdata,idx['labels']],axis = 1)\n",
    "    ###spliting rawdata into training and validation based on label column \n",
    "    grouped_rawdata = rawdata_con.groupby('labels')\n",
    "    training = grouped_rawdata.get_group('training')\n",
    "    validation = grouped_rawdata.get_group('validation')\n",
    "    return training,validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `build_model` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    \"\"\"Build and compile the NN model\"\"\"\n",
    "    model = tf.keras.Sequential([\n",
    "       tf.keras.layers.Dense(\n",
    "                        units=hp.Int('units',\n",
    "                        min_value=2,\n",
    "                        max_value=14,\n",
    "                        step=1),\n",
    "                        activation=hp.Choice(\"activation\", [\"relu\", \"tanh\"])\n",
    "                        ),\n",
    "        tf.keras.layers.Dense(units=1)\n",
    "        ])\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "            loss='mse',\n",
    "            metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define `wettodry` and `drytowet` functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wettodry(vwc):\n",
    "    vwc = [0 if i < 0 else i for i in vwc]\n",
    "    w2d = [vwc[0]]\n",
    "    for idx in range(1, len(vwc)):\n",
    "        if vwc[idx] < w2d[idx-1]:\n",
    "            w2d.append(vwc[idx])\n",
    "        else:\n",
    "            w2d.append(w2d[idx-1])\n",
    "    return w2d\n",
    "\n",
    "def drytowet(vwc):\n",
    "    vwc = [0 if i < 0 else i for i in vwc]\n",
    "    rslt_reverse = vwc[::-1]\n",
    "    d2w = [rslt_reverse[0]]\n",
    "    for idx in range(1, len(vwc)):\n",
    "        if rslt_reverse[idx] > d2w[idx-1]:\n",
    "            d2w.append(rslt_reverse[idx])\n",
    "        else:\n",
    "            d2w.append(d2w[idx-1])\n",
    "    return d2w[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define `bag_predict`\n",
    "and return mean and standard deviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_predict(models, X_test):\n",
    "    bag_pred = pd.DataFrame()\n",
    "    for ann in models:\n",
    "        # print(ann)\n",
    "        ann_all = tf.keras.models.load_model(ann)\n",
    "        y_pred = ann_all.predict(X_test)\n",
    "        w2d = wettodry(y_pred.ravel().tolist())\n",
    "        d2w = drytowet(y_pred.ravel().tolist())\n",
    "        # print(w2d==d2w)\n",
    "        pred_list = [(g + h) / 2 for g, h in zip(w2d, d2w)]\n",
    "        bag_pred[ann[ann.find('ann'):-3]] = pd.Series(pred_list)\n",
    "\n",
    "    mean_vwc = bag_pred.mean(axis=1)\n",
    "    std_vwc = bag_pred.std(axis=1)\n",
    "    return mean_vwc, std_vwc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define `plot_swrc` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrap (finding and fitting best models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Complete [00h 01m 17s]\n",
      "val_loss: 0.003532117698341608\n",
      "\n",
      "Best val_loss So Far: 0.003046780126169324\n",
      "Total elapsed time: 00h 06m 02s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "Fitting the best model for iteration 100.......\n",
      "Training Size = 12859, validation Size = 7524\n",
      "Unique soils in Training = 151\n",
      "Unique soils in Validation = 81\n",
      "INPUT SHAPE: 5\n",
      "Activation function --- <function tanh at 0x000001D3E7A52E50>\n",
      "Number of hidden neurons = 12\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "losses = {}\n",
    "\n",
    "iteration = 100\n",
    "for bb in range(iteration):\n",
    "    training, validation = bagging(data)\n",
    "    ## MODEL 1\n",
    "    # X_train = training.iloc[:,2:-2]\n",
    "    # X_valid = validation.iloc[:,2:-2]\n",
    "    ## MODEL 2\n",
    "    # X_train = training.loc[:,['clay', 'silt', 'sand','pF']]\n",
    "    # X_valid = validation.loc[:,['clay', 'silt', 'sand','pF']]\n",
    "    ## MODEL 3\n",
    "    # X_train = training.loc[:,['clay', 'silt', 'sand','BD', 'pF']]\n",
    "    # X_valid = validation.loc[:,['clay', 'silt', 'sand', 'BD', 'pF']]\n",
    "    ## MODEL 4\n",
    "    X_train = training.loc[:,['clay', 'silt', 'sand','omc', 'pF']]\n",
    "    X_valid = validation.loc[:,['clay', 'silt', 'sand', 'omc', 'pF']]\n",
    "\n",
    "    y_train = training.iloc[:,-2]\n",
    "    y_valid = validation.iloc[:,-2]\n",
    "\n",
    "    # scaler = MinMaxScaler() \n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_valid = scaler.transform(X_valid)\n",
    "\n",
    "    tuner = RandomSearch(\n",
    "        build_model,    \n",
    "        objective='val_loss',\n",
    "        max_trials=5,\n",
    "        executions_per_trial=1,\n",
    "        overwrite=True,\n",
    "        directory='project',\n",
    "        project_name='HPtuning_'+str(bb))\n",
    "\n",
    "    early_stop = [tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",\n",
    "                    patience=4,\n",
    "                    restore_best_weights=True)]\n",
    "\n",
    "    tuner.search(X_train, y_train,\n",
    "                epochs=25,\n",
    "                validation_data=(X_valid, y_valid),\n",
    "                callbacks=early_stop,\n",
    "                verbose=2)\n",
    "\n",
    "    print(f\"Fitting the best model for iteration {bb+1}.......\" \"\\n\"\n",
    "            f\"Training Size = {len(training)}, validation Size = {len(validation)}\" \"\\n\"\n",
    "            f\"Unique soils in Training = {len(training.no.unique())}\" \"\\n\"\n",
    "            f\"Unique soils in Validation = {len(validation.no.unique())}\" \"\\n\"\n",
    "            f\"INPUT SHAPE: {X_train.shape[1]}\")\n",
    "\n",
    "    best_hps = tuner.get_best_hyperparameters(2)\n",
    "    model = build_model(best_hps[0])\n",
    "\n",
    "    callback = [tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",\n",
    "                    patience=15,\n",
    "                    restore_best_weights=True),\n",
    "                tf.keras.callbacks.ModelCheckpoint(filepath=\"model4/ann_\"+str(bb)+\".h5\",\n",
    "                    monitor=\"val_loss\",\n",
    "                    save_best_only=True)]\n",
    "    # use `model.fit()` here to train and save the best model from `tuner.search`\n",
    "    history = model.fit(X_train, y_train,\n",
    "                batch_size = 32, epochs = 500,\n",
    "                callbacks=callback,\n",
    "                validation_data=(X_valid, y_valid),\n",
    "                verbose=0)\n",
    "    losses[bb] = history.history\n",
    "    print(f\"Activation function --- {model.layers[0].activation}\"\"\\n\"\n",
    "        f\"Number of hidden neurons = {model.layers[0].units}\")\n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('history_4.pkl', 'wb') as handle:\n",
    "    pickle.dump(losses, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# with open('best_HPall_b.pkl.pkl', 'rb') as handle:\n",
    "#     best_parms = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function tanh at 0x000001D3E7A52E50> 11\n",
      "<function tanh at 0x000001D3E7A52E50> 13\n",
      "<function tanh at 0x000001D3E7A52E50> 6\n",
      "<function relu at 0x000001D3E7A52AF0> 6\n",
      "<function tanh at 0x000001D3E7A52E50> 12\n",
      "<function relu at 0x000001D3E7A52AF0> 13\n",
      "<function relu at 0x000001D3E7A52AF0> 13\n",
      "<function relu at 0x000001D3E7A52AF0> 12\n",
      "<function tanh at 0x000001D3E7A52E50> 10\n",
      "<function relu at 0x000001D3E7A52AF0> 8\n",
      "<function tanh at 0x000001D3E7A52E50> 13\n",
      "<function tanh at 0x000001D3E7A52E50> 8\n",
      "<function relu at 0x000001D3E7A52AF0> 12\n",
      "<function relu at 0x000001D3E7A52AF0> 13\n",
      "<function relu at 0x000001D3E7A52AF0> 8\n",
      "<function tanh at 0x000001D3E7A52E50> 8\n",
      "<function tanh at 0x000001D3E7A52E50> 3\n",
      "<function relu at 0x000001D3E7A52AF0> 12\n",
      "<function relu at 0x000001D3E7A52AF0> 8\n",
      "<function tanh at 0x000001D3E7A52E50> 14\n",
      "<function tanh at 0x000001D3E7A52E50> 11\n",
      "<function tanh at 0x000001D3E7A52E50> 11\n",
      "<function relu at 0x000001D3E7A52AF0> 12\n",
      "<function relu at 0x000001D3E7A52AF0> 11\n",
      "<function relu at 0x000001D3E7A52AF0> 13\n",
      "<function relu at 0x000001D3E7A52AF0> 7\n",
      "<function relu at 0x000001D3E7A52AF0> 14\n",
      "<function relu at 0x000001D3E7A52AF0> 8\n",
      "<function tanh at 0x000001D3E7A52E50> 14\n",
      "<function tanh at 0x000001D3E7A52E50> 12\n",
      "<function tanh at 0x000001D3E7A52E50> 10\n",
      "<function tanh at 0x000001D3E7A52E50> 10\n",
      "<function tanh at 0x000001D3E7A52E50> 4\n",
      "<function relu at 0x000001D3E7A52AF0> 13\n",
      "<function tanh at 0x000001D3E7A52E50> 8\n",
      "<function tanh at 0x000001D3E7A52E50> 12\n",
      "<function relu at 0x000001D3E7A52AF0> 4\n",
      "<function tanh at 0x000001D3E7A52E50> 9\n",
      "<function tanh at 0x000001D3E7A52E50> 6\n",
      "<function tanh at 0x000001D3E7A52E50> 9\n",
      "<function relu at 0x000001D3E7A52AF0> 13\n",
      "<function tanh at 0x000001D3E7A52E50> 4\n",
      "<function tanh at 0x000001D3E7A52E50> 4\n",
      "<function tanh at 0x000001D3E7A52E50> 12\n",
      "<function relu at 0x000001D3E7A52AF0> 11\n",
      "<function tanh at 0x000001D3E7A52E50> 12\n",
      "<function tanh at 0x000001D3E7A52E50> 10\n",
      "<function relu at 0x000001D3E7A52AF0> 12\n",
      "<function tanh at 0x000001D3E7A52E50> 13\n",
      "<function tanh at 0x000001D3E7A52E50> 11\n",
      "<function relu at 0x000001D3E7A52AF0> 7\n",
      "<function tanh at 0x000001D3E7A52E50> 14\n",
      "<function tanh at 0x000001D3E7A52E50> 12\n",
      "<function tanh at 0x000001D3E7A52E50> 8\n",
      "<function tanh at 0x000001D3E7A52E50> 11\n",
      "<function tanh at 0x000001D3E7A52E50> 12\n",
      "<function tanh at 0x000001D3E7A52E50> 11\n",
      "<function tanh at 0x000001D3E7A52E50> 3\n",
      "<function tanh at 0x000001D3E7A52E50> 9\n",
      "<function relu at 0x000001D3E7A52AF0> 10\n",
      "<function relu at 0x000001D3E7A52AF0> 12\n",
      "<function tanh at 0x000001D3E7A52E50> 11\n",
      "<function relu at 0x000001D3E7A52AF0> 11\n",
      "<function tanh at 0x000001D3E7A52E50> 11\n",
      "<function relu at 0x000001D3E7A52AF0> 12\n",
      "<function tanh at 0x000001D3E7A52E50> 12\n",
      "<function relu at 0x000001D3E7A52AF0> 3\n",
      "<function relu at 0x000001D3E7A52AF0> 9\n",
      "<function tanh at 0x000001D3E7A52E50> 11\n",
      "<function relu at 0x000001D3E7A52AF0> 11\n",
      "<function tanh at 0x000001D3E7A52E50> 7\n",
      "<function tanh at 0x000001D3E7A52E50> 5\n",
      "<function tanh at 0x000001D3E7A52E50> 10\n",
      "<function tanh at 0x000001D3E7A52E50> 13\n",
      "<function relu at 0x000001D3E7A52AF0> 9\n",
      "<function tanh at 0x000001D3E7A52E50> 6\n",
      "<function tanh at 0x000001D3E7A52E50> 9\n",
      "<function tanh at 0x000001D3E7A52E50> 12\n",
      "<function relu at 0x000001D3E7A52AF0> 11\n",
      "<function relu at 0x000001D3E7A52AF0> 9\n",
      "<function tanh at 0x000001D3E7A52E50> 8\n",
      "<function relu at 0x000001D3E7A52AF0> 12\n",
      "<function tanh at 0x000001D3E7A52E50> 10\n",
      "<function tanh at 0x000001D3E7A52E50> 8\n",
      "<function tanh at 0x000001D3E7A52E50> 10\n",
      "<function tanh at 0x000001D3E7A52E50> 13\n",
      "<function relu at 0x000001D3E7A52AF0> 14\n",
      "<function relu at 0x000001D3E7A52AF0> 8\n",
      "<function tanh at 0x000001D3E7A52E50> 8\n",
      "<function relu at 0x000001D3E7A52AF0> 8\n",
      "<function tanh at 0x000001D3E7A52E50> 7\n",
      "<function relu at 0x000001D3E7A52AF0> 12\n",
      "<function tanh at 0x000001D3E7A52E50> 12\n",
      "<function tanh at 0x000001D3E7A52E50> 6\n",
      "<function relu at 0x000001D3E7A52AF0> 4\n",
      "<function relu at 0x000001D3E7A52AF0> 7\n",
      "<function tanh at 0x000001D3E7A52E50> 7\n",
      "<function relu at 0x000001D3E7A52AF0> 8\n",
      "<function relu at 0x000001D3E7A52AF0> 14\n",
      "<function tanh at 0x000001D3E7A52E50> 12\n"
     ]
    }
   ],
   "source": [
    "models = ['model4/ann_'+ str(i) + '.h5' for i in range(100)]\n",
    "hidden_n, activation_fn = [] ,[]\n",
    "for ann in models:\n",
    "    model = tf.keras.models.load_model(ann)\n",
    "    print(model.layers[0].activation, model.layers[0].units)\n",
    "    # hidden_n.append(model.layers[0].units), activation_fn.append(model.layers[0].activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data and apply Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = os.path.abspath('PTF3_data.xlsx')\n",
    "\n",
    "data2 = pd.read_excel(file,'lab')\n",
    "data2 = data2.head(1016).iloc[:,:17]\n",
    "data2.columns, data2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save Scaler\n",
    "scaler = StandardScaler()\n",
    "##MODEL 1\n",
    "# scaler.fit_transform(data.iloc[:,2:-1])\n",
    "# pickle.dump(scaler, open('ann1_stdscaler.pkl', 'wb'))\n",
    "\n",
    "##MODEL 2\n",
    "scaler.fit_transform(data.loc[:,['clay', 'silt', 'sand', 'omc', 'pF']])\n",
    "pickle.dump(scaler, open('ann4_stdscaler.pkl', 'wb'))\n",
    "\n",
    "## MODEL 3\n",
    "# scaler.fit_transform(data.loc[:,['clay', 'silt', 'sand','BD', 'pF']])\n",
    "# pickle.dump(scaler, open('ann3_stdscaler.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = pickle.load(open('ann1_stdscaler.pkl', 'rb'))\n",
    "# scaler = pickle.load(open('ann3_stdscaler.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_Xtest(data):\n",
    "    \"\"\"enter the dataframe and this function will return the X_test data\"\"\"\n",
    "    pF = pd.Series(np.arange(-1, 6, 0.05), name='pF')\n",
    "    df = pd.concat([data.drop_duplicates()]*len(pF), ignore_index=True)\n",
    "    test = pd.concat([df,pF], axis=1)\n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colList = ['no', 'clay', 'silt', 'sand']\n",
    "df_group = data[colList].groupby(['no'])\n",
    "test_all = df_group.apply(create_Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_soil = random.choice(data.no.unique())\n",
    "print(rand_soil)\n",
    "soil_test = test_all[test_all['no']==rand_soil]\n",
    "\n",
    "X_test = scaler.transform(soil_test.iloc[:,1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have to use `bag_predict` and scaler on each soil individually.\n",
    "Making predicitons on the entire dataset results in wrong results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply `bag_predict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['model2/ann_'+ str(i) + '.h5' for i in range(100)]\n",
    "# mean_vwc, std_vwc = bag_predict(models, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_all = pd.concat([test_all.reset_index(drop=True), pd.Series(mean_vwc, name = 'mean_vwc'),\n",
    "            pd.Series(std_vwc, name = 'std_vwc')], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.5, style=\"ticks\")\n",
    "import random\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(2):\n",
    "    ax = plt.subplot(1, 2, i + 1)\n",
    "    rand_soil = random.choice(data.no.unique())\n",
    "    soil_test = test_all[test_all['no']==rand_soil]\n",
    "    soil_df = data[data['no']==rand_soil]\n",
    "    X_test = scaler.transform(soil_test.iloc[:,1:])\n",
    "    mean_vwc, std_vwc = bag_predict(models, X_test)\n",
    "\n",
    "    ax.plot(soil_df['pF'], soil_df['VWC'], 'o', alpha=0.6, label = 'VWC')\n",
    "    ax.plot(soil_test['pF'], mean_vwc, '-', label = 'ANN')\n",
    "    ax.fill_between(soil_test['pF'], mean_vwc+std_vwc, mean_vwc-std_vwc, alpha=0.3)\n",
    "    ax.set_xlim(0,5)\n",
    "    ax.legend(prop={'size': 14})\n",
    "    ax.grid(True,linestyle='--')\n",
    "    ax.xaxis.set_major_locator(MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(MultipleLocator(.1))\n",
    "    ax.yaxis.set_minor_locator(AutoMinorLocator())\n",
    "    ax.xaxis.set_minor_locator(AutoMinorLocator())\n",
    "    ax.set_xlabel('pF',size = 18, weight = 'bold')\n",
    "    # ax.set_ylabel('VWC'r' [$cm^3 cm^{-3}$]',size = 18, weight = 'bold')\n",
    "    ax.set_title(rand_soil)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Miscellaneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotly.graph_objects as go\n",
    "# fig = go.Figure()\n",
    "# fig.add_trace(go.Scatter(x=soil_df[\"pF\"], y=soil_df[\"VWC\"]/100,\n",
    "#                         mode='markers',\n",
    "#                         name='VWC'))\n",
    "# fig.add_trace(go.Scatter(x=pF, y=mean_vwc,\n",
    "#                         mode='lines',\n",
    "#                         name='ANN_all'))\n",
    "# fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5a31b28fac5f527939e377b0177a44b7cc412165df50679a76816075476fa329"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
